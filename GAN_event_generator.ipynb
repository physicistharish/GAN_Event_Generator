{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GAN_event_generator.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/physicistharish/GAN_Event_Generator/blob/main/GAN_event_generator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6mAFVt4RgNcU"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import glob\n",
        "import imageio\n",
        "import matplotlib.pyplot as plt\n",
        "import PIL\n",
        "from tensorflow.keras import layers\n",
        "import time\n",
        "import io\n",
        "import pickle\n",
        "from IPython import display\n",
        "from google.colab import files"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ht_zCXy4Dhnm"
      },
      "source": [
        "from keras import backend\n",
        "\n",
        "from numpy import cov\n",
        "from numpy import trace\n",
        "from numpy import iscomplexobj\n",
        "from numpy.random import random\n",
        "from scipy.linalg import sqrtm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P9qodlS4ALYK",
        "outputId": "9cd21e7e-63d9-4d75-9810-646b327f82be"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "path = \"/content/drive/MyDrive/Project_Files/Csv/16349 events XY.csv\"# This line is not required if the input dataset has the proper 13 columns\n",
        "\n",
        "df1 = pd.read_csv(path)\n",
        "print(df1)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "       Unnamed: 0         0         1  ...        10        11        12\n",
            "0               0  0.388266  1.066190  ...  -12.1232  -36.5231  -76.7150\n",
            "1               0 -0.450752 -0.833231  ...   24.5506   32.9202   40.7282\n",
            "2               0  1.080110  2.199080  ...   42.8811   61.5420   89.7588\n",
            "3               0 -2.602280 -7.072710  ...  -40.4210  -47.4739  -66.2680\n",
            "4               0  0.085928  0.266413  ...  108.9250  105.9890   95.7408\n",
            "...           ...       ...       ...  ...       ...       ...       ...\n",
            "16344           0  0.424090  0.911776  ...   17.8851   16.7183   18.3112\n",
            "16345           0  0.210162  2.838150  ...   20.9781   24.7027   34.6982\n",
            "16346           0  0.570230  1.158780  ...   69.3984   86.1083  108.1730\n",
            "16347           0  0.645774  2.287450  ...   17.2922   14.9753  -19.0565\n",
            "16348           0  0.895050  3.375180  ...   19.4355   31.8020   33.9234\n",
            "\n",
            "[16349 rows x 14 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GsHEBXIIK_Eu",
        "outputId": "c097115f-36b7-4e36-9484-45386a682b94"
      },
      "source": [
        "df1 = df1.drop(columns='Unnamed: 0') # This line is not required if the input dataset has the proper 13 columns\n",
        "print(df1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              0         1         2  ...        10        11        12\n",
            "0      0.388266  1.066190   1.65756  ...  -12.1232  -36.5231  -76.7150\n",
            "1     -0.450752 -0.833231  -1.64948  ...   24.5506   32.9202   40.7282\n",
            "2      1.080110  2.199080   4.15661  ...   42.8811   61.5420   89.7588\n",
            "3     -2.602280 -7.072710 -10.71260  ...  -40.4210  -47.4739  -66.2680\n",
            "4      0.085928  0.266413   2.94087  ...  108.9250  105.9890   95.7408\n",
            "...         ...       ...       ...  ...       ...       ...       ...\n",
            "16344  0.424090  0.911776   1.41807  ...   17.8851   16.7183   18.3112\n",
            "16345  0.210162  2.838150   7.73274  ...   20.9781   24.7027   34.6982\n",
            "16346  0.570230  1.158780   1.30524  ...   69.3984   86.1083  108.1730\n",
            "16347  0.645774  2.287450   2.81709  ...   17.2922   14.9753  -19.0565\n",
            "16348  0.895050  3.375180   3.97640  ...   19.4355   31.8020   33.9234\n",
            "\n",
            "[16349 rows x 13 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5nSJF6aOchNx"
      },
      "source": [
        "def z_score(df):\n",
        "    # copy the dataframe\n",
        "    df_std = df.copy()\n",
        "    df_std = df_std.transpose()\n",
        "    # apply the z-score method\n",
        "    for column in df_std.columns:\n",
        "        df_std[column] = (df_std[column] - df_std[column].mean()) / df_std[column].std()\n",
        "    df_std = df_std.transpose()   \n",
        "    return df_std"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z71_tP5flZ4q"
      },
      "source": [
        "df1=z_score(df1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N2Xou1cAliEK"
      },
      "source": [
        "BUFFER_SIZE = 16300\n",
        "BATCH_SIZE = 1\n",
        "\n",
        "train_dataset_new = tf.data.Dataset.from_tensor_slices(df1).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lV_MBL1Jl6h8"
      },
      "source": [
        "def make_generator_model():\n",
        "    model = tf.keras.Sequential()\n",
        "    \n",
        "    model.add(layers.Dense(4, use_bias=False, input_shape=(100,)))\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    model.add(layers.Dense(2*64,use_bias=False))\n",
        "    model.add(layers.LeakyReLU())\n",
        "    model.add(layers.Dropout(0.2))\n",
        "\n",
        "    model.add(layers.Dense(2*64,use_bias=False))\n",
        "    model.add(layers.LeakyReLU())\n",
        "    model.add(layers.Dropout(0.2))\n",
        "    \n",
        "    model.add(layers.Dense(13,use_bias=False))\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    model.add(layers.Reshape((13,1)))\n",
        "    \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YLkweKKemQIX"
      },
      "source": [
        "generator = make_generator_model()\n",
        "# Trying out a sample generation\n",
        "noise = tf.random.normal([1, 100])\n",
        "generated_image = generator(noise, training=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7LY7LFDFtAGV"
      },
      "source": [
        "def make_discriminator_model():\n",
        "    model = tf.keras.Sequential()\n",
        "\n",
        "    model.add(layers.Flatten())\n",
        "    model.add(layers.Dense(13,use_bias=False))\n",
        "    model.add(layers.LeakyReLU())\n",
        "    \n",
        "    model.add(layers.Dense(64,use_bias=False))\n",
        "    model.add(layers.LeakyReLU())\n",
        "    model.add(layers.Dropout(0.2))\n",
        "      \n",
        "    model.add(layers.Dense(64,use_bias=False))\n",
        "    model.add(layers.LeakyReLU())\n",
        "    model.add(layers.Dropout(0.2))\n",
        "   \n",
        "    model.add(layers.Dense(1,use_bias=False))\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qqLVAzQQzXhx"
      },
      "source": [
        "discriminator = make_discriminator_model()\n",
        "decision = discriminator(generated_image)\n",
        "print (decision)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m0OQY27bzlU9"
      },
      "source": [
        "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hM9dqBUIz0tq"
      },
      "source": [
        "def discriminator_loss(real_output, fake_output):\n",
        "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
        "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
        "    total_loss = real_loss + fake_loss\n",
        "    return total_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBrCWW_a0KIK"
      },
      "source": [
        "def generator_loss(fake_output):\n",
        "    return cross_entropy(tf.ones_like(fake_output), fake_output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJoBUXGV0MQ2"
      },
      "source": [
        "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X93srOkw0OVe"
      },
      "source": [
        "EPOCHS = 15\n",
        "noise_dim = 100\n",
        "num_examples_to_generate = 10\n",
        "\n",
        "seed = tf.random.normal([num_examples_to_generate, noise_dim])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ri93N4c0WLW"
      },
      "source": [
        "@tf.function\n",
        "def train_step(images,n):\n",
        "    noise = tf.random.normal([BATCH_SIZE, noise_dim])\n",
        "\n",
        "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "      generated_images = generator(noise, training=True)\n",
        "\n",
        "      real_output = discriminator(images, training=True)\n",
        "      fake_output = discriminator(generated_images, training=True)\n",
        "      \n",
        "      gen_loss = generator_loss(fake_output)\n",
        "      disc_loss = discriminator_loss(real_output, fake_output)\n",
        "      \n",
        "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
        "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
        "\n",
        "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
        "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GkEBWU352ZoP"
      },
      "source": [
        "def train(dataset, epochs):\n",
        "  for i in range(epochs):\n",
        "    start = time.time()\n",
        "\n",
        "    for image_batch in dataset:\n",
        "      train_step(image_batch,i)\n",
        "\n",
        "    pddf = generate_and_save_images(generator, i + 1,seed)\n",
        "    print('Time for epoch {} is {} sec'.format(i + 1, time.time()-start))\n",
        "    if i == epochs-1:\n",
        "      pddf.to_csv('file_name.csv'.format(i+1),mode = 'a')\n",
        "      files.download('file_name.csv'.format(i+1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rNEj_0Y05PrC"
      },
      "source": [
        "def generate_and_save_images(model, test_input):\n",
        "  predictions = model(test_input, training=False)\n",
        "   \n",
        "  predictions = tf.reshape(predictions,[num_examples_to_generate,13])\n",
        "  predict_data = predictions.numpy()\n",
        "  panda_df = pd.DataFrame(data = predict_data)\n",
        "\n",
        "  return panda_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8WoXKR3n6STa"
      },
      "source": [
        "train(train_dataset_new, EPOCHS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LV8Sui24QSHl"
      },
      "source": [
        "new_df = generate_and_save_images(generator,tf.random.normal([20000,100]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tuDz0YJLQj3J"
      },
      "source": [
        "new_df.to_csv('20000 generated posx.csv')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
